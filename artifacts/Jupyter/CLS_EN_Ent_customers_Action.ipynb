{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1V0nYGgT",
   "metadata": {},
   "outputs": [],
   "source" : [
    "# Setup input parameters\n",
    "from datetime import datetime as dt\n",
    "dbutils.widgets.text('bg_loadtimestamp', '')\n",
    "bg_loadtimestamp = dbutils.widgets.get('bg_loadtimestamp')\n",
    "bg_loadtimestamp_str = bg_loadtimestamp\n",
    "if not bg_loadtimestamp:\n",
    "    bg_loadtimestamp = 'CAST(NULL AS Timestamp)'\n",
    "else:\n",
    "    bg_loadtimestamp = f\"CAST('{bg_loadtimestamp}' AS Timestamp)\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "J0Zbs5BJ",
   "metadata": {},
   "outputs": [],
   "source" : [
    "# Setup logging\n",
    "logger = spark._jvm.org.apache.log4j.Logger.getLogger('com.bigenius-x.application')\n",
    "def info(targetName, message):\n",
    "    logger.info(f'{targetName}: {message}')\n",
    "    print(f\"{dt.now().strftime('%Y/%m/%d, %H:%M:%S')} - {targetName}: {message}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dykxQ+Mp",
   "metadata": {},
   "outputs": [],
   "source" : [
    "# EntityCleanseAction: Ent_customers_Entity Cleanse Action_1\n",
    "\n",
    "try:\n",
    "\n",
    "    operation_metrics_collection = {}\n",
    "    spark.sql(\"\"\"DROP TABLE IF EXISTS `{marc_databricks_initiative#cleanse#database_name}`.`{marc_databricks_initiative#cleanse#schema_name}`.`bg_erroneousrows_ent_customers_dataflow1`\"\"\")\n",
    "\n",
    "    spark.sql(\"\"\"CREATE TABLE `{marc_databricks_initiative#cleanse#database_name}`.`{marc_databricks_initiative#cleanse#schema_name}`.`bg_erroneousrows_ent_customers_dataflow1` (\n",
    "         `bg_cleanse_id` Long NOT NULL\n",
    "    )\n",
    "    USING delta\n",
    "    \"\"\"\n",
    "    )\n",
    "\n",
    "    spark.sql(\"\"\"DROP TABLE IF EXISTS `{marc_databricks_initiative#cleanse#database_name}`.`{marc_databricks_initiative#cleanse#schema_name}`.`bg_excludedrows_ent_customers_dataflow1`\"\"\")\n",
    "\n",
    "    spark.sql(\"\"\"CREATE TABLE `{marc_databricks_initiative#cleanse#database_name}`.`{marc_databricks_initiative#cleanse#schema_name}`.`bg_excludedrows_ent_customers_dataflow1` (\n",
    "         `bg_cleanse_id` Long NOT NULL\n",
    "    )\n",
    "    USING delta\n",
    "    \"\"\"\n",
    "    )\n",
    "\n",
    "    result_df = spark.sql(f\"\"\"\n",
    "    INSERT\n",
    "    OVERWRITE `{marc_databricks_initiative#cleanse#database_name}`.`{marc_databricks_initiative#cleanse#schema_name}`.`bg_erroneousrows_ent_customers_dataflow1` (\n",
    "         `bg_cleanse_id`\n",
    "    )\n",
    "    SELECT\n",
    "         `bg_cleanse_id`\n",
    "    FROM `{marc_databricks_initiative#cleanse#database_name}`.`{marc_databricks_initiative#cleanse#schema_name}`.`cls_en_ent_customers`\n",
    "    WHERE `customer_id` IS NULL\n",
    "    \"\"\")\n",
    "    result_df = spark.sql(f\"\"\"\n",
    "    INSERT\n",
    "    INTO `{marc_databricks_initiative#cleanse#database_name}`.`{marc_databricks_initiative#cleanse#schema_name}`.`cls_en_ent_customers_error` (\n",
    "         `bg_sourcesystem`\n",
    "        ,`bg_loadtimestamp`\n",
    "        ,`bg_errorcount`\n",
    "        ,`bg_errordescription`\n",
    "        ,`bg_effectivetimestamp`\n",
    "        ,`bg_cleanse_id`\n",
    "        ,`customer_id`\n",
    "        ,`email_address`\n",
    "        ,`full_name`\n",
    "        ,`email_address1`\n",
    "        ,`full_name1`\n",
    "    )\n",
    "    SELECT\n",
    "         `bg_source`.`bg_sourcesystem` AS `bg_sourcesystem`\n",
    "        ,`bg_source`.`bg_loadtimestamp` AS `bg_loadtimestamp`\n",
    "        ,1 AS `bg_errorcount`\n",
    "        ,'Nulled non-nullable business key \"customer_id\"' AS `bg_errordescription`\n",
    "        ,`bg_source`.`bg_effectivetimestamp` AS `bg_effectivetimestamp`\n",
    "        ,`bg_source`.`bg_cleanse_id` AS `bg_cleanse_id`\n",
    "        ,`bg_source`.`customer_id` AS `customer_id`\n",
    "        ,`bg_source`.`email_address` AS `email_address`\n",
    "        ,`bg_source`.`full_name` AS `full_name`\n",
    "        ,`bg_source`.`email_address1` AS `email_address1`\n",
    "        ,`bg_source`.`full_name1` AS `full_name1`\n",
    "    FROM `{marc_databricks_initiative#cleanse#database_name}`.`{marc_databricks_initiative#cleanse#schema_name}`.`cls_en_ent_customers` AS `bg_source`\n",
    "    JOIN `{marc_databricks_initiative#cleanse#database_name}`.`{marc_databricks_initiative#cleanse#schema_name}`.`bg_erroneousrows_ent_customers_dataflow1` AS `bg_error`\n",
    "       ON `bg_source`.`bg_cleanse_id` = `bg_error`.`bg_cleanse_id`\n",
    "    \"\"\")\n",
    "    result_df = spark.sql(f\"\"\"\n",
    "    INSERT\n",
    "    INTO `{marc_databricks_initiative#cleanse#database_name}`.`{marc_databricks_initiative#cleanse#schema_name}`.`bg_excludedrows_ent_customers_dataflow1` (\n",
    "         `bg_cleanse_id`\n",
    "    )\n",
    "    SELECT\n",
    "         `bg_cleanse_id`\n",
    "    FROM `{marc_databricks_initiative#cleanse#database_name}`.`{marc_databricks_initiative#cleanse#schema_name}`.`bg_erroneousrows_ent_customers_dataflow1`\n",
    "    \"\"\")\n",
    "    RowCountInserted = result_df.select(\"num_inserted_rows\").collect()[0][0]\n",
    "    operation_metrics = result_df.toJSON().collect()\n",
    "    operation_metrics_collection['detectnonnullablenulledbusinesskeys_insertexcludedrows_{marc_databricks_initiative#cleanse#database_name}_{marc_databricks_initiative#cleanse#schema_name}_bg_excludedrows_ent_customers_dataflow1'] = operation_metrics\n",
    "    result_df = spark.sql(f\"\"\"\n",
    "    INSERT\n",
    "    OVERWRITE `{marc_databricks_initiative#cleanse#database_name}`.`{marc_databricks_initiative#cleanse#schema_name}`.`bg_erroneousrows_ent_customers_dataflow1` (\n",
    "         `bg_cleanse_id`\n",
    "    )\n",
    "    SELECT\n",
    "         `bg_cleanse_id`\n",
    "    FROM `{marc_databricks_initiative#cleanse#database_name}`.`{marc_databricks_initiative#cleanse#schema_name}`.`cls_en_ent_customers`\n",
    "    WHERE `email_address` IS NULL\n",
    "    \"\"\")\n",
    "    result_df = spark.sql(f\"\"\"\n",
    "    INSERT\n",
    "    INTO `{marc_databricks_initiative#cleanse#database_name}`.`{marc_databricks_initiative#cleanse#schema_name}`.`cls_en_ent_customers_error` (\n",
    "         `bg_sourcesystem`\n",
    "        ,`bg_loadtimestamp`\n",
    "        ,`bg_errorcount`\n",
    "        ,`bg_errordescription`\n",
    "        ,`bg_effectivetimestamp`\n",
    "        ,`bg_cleanse_id`\n",
    "        ,`customer_id`\n",
    "        ,`email_address`\n",
    "        ,`full_name`\n",
    "        ,`email_address1`\n",
    "        ,`full_name1`\n",
    "    )\n",
    "    SELECT\n",
    "         `bg_source`.`bg_sourcesystem` AS `bg_sourcesystem`\n",
    "        ,`bg_source`.`bg_loadtimestamp` AS `bg_loadtimestamp`\n",
    "        ,1 AS `bg_errorcount`\n",
    "        ,'Nulled non-nullable business key \"email_address\"' AS `bg_errordescription`\n",
    "        ,`bg_source`.`bg_effectivetimestamp` AS `bg_effectivetimestamp`\n",
    "        ,`bg_source`.`bg_cleanse_id` AS `bg_cleanse_id`\n",
    "        ,`bg_source`.`customer_id` AS `customer_id`\n",
    "        ,`bg_source`.`email_address` AS `email_address`\n",
    "        ,`bg_source`.`full_name` AS `full_name`\n",
    "        ,`bg_source`.`email_address1` AS `email_address1`\n",
    "        ,`bg_source`.`full_name1` AS `full_name1`\n",
    "    FROM `{marc_databricks_initiative#cleanse#database_name}`.`{marc_databricks_initiative#cleanse#schema_name}`.`cls_en_ent_customers` AS `bg_source`\n",
    "    JOIN `{marc_databricks_initiative#cleanse#database_name}`.`{marc_databricks_initiative#cleanse#schema_name}`.`bg_erroneousrows_ent_customers_dataflow1` AS `bg_error`\n",
    "       ON `bg_source`.`bg_cleanse_id` = `bg_error`.`bg_cleanse_id`\n",
    "    \"\"\")\n",
    "    result_df = spark.sql(f\"\"\"\n",
    "    INSERT\n",
    "    INTO `{marc_databricks_initiative#cleanse#database_name}`.`{marc_databricks_initiative#cleanse#schema_name}`.`bg_excludedrows_ent_customers_dataflow1` (\n",
    "         `bg_cleanse_id`\n",
    "    )\n",
    "    SELECT\n",
    "         `bg_cleanse_id`\n",
    "    FROM `{marc_databricks_initiative#cleanse#database_name}`.`{marc_databricks_initiative#cleanse#schema_name}`.`bg_erroneousrows_ent_customers_dataflow1`\n",
    "    \"\"\")\n",
    "    RowCountInserted = result_df.select(\"num_inserted_rows\").collect()[0][0]\n",
    "    operation_metrics = result_df.toJSON().collect()\n",
    "    operation_metrics_collection['detectnonnullablenulledbusinesskeys_insertexcludedrows_{marc_databricks_initiative#cleanse#database_name}_{marc_databricks_initiative#cleanse#schema_name}_bg_excludedrows_ent_customers_dataflow1'] = operation_metrics\n",
    "    result_df = spark.sql(f\"\"\"\n",
    "    INSERT\n",
    "    OVERWRITE `{marc_databricks_initiative#cleanse#database_name}`.`{marc_databricks_initiative#cleanse#schema_name}`.`bg_erroneousrows_ent_customers_dataflow1` (\n",
    "         `bg_cleanse_id`\n",
    "    )\n",
    "    SELECT\n",
    "         `bg_cleanse_id`\n",
    "    FROM `{marc_databricks_initiative#cleanse#database_name}`.`{marc_databricks_initiative#cleanse#schema_name}`.`cls_en_ent_customers`\n",
    "    WHERE `full_name` IS NULL\n",
    "    \"\"\")\n",
    "    result_df = spark.sql(f\"\"\"\n",
    "    INSERT\n",
    "    INTO `{marc_databricks_initiative#cleanse#database_name}`.`{marc_databricks_initiative#cleanse#schema_name}`.`cls_en_ent_customers_error` (\n",
    "         `bg_sourcesystem`\n",
    "        ,`bg_loadtimestamp`\n",
    "        ,`bg_errorcount`\n",
    "        ,`bg_errordescription`\n",
    "        ,`bg_effectivetimestamp`\n",
    "        ,`bg_cleanse_id`\n",
    "        ,`customer_id`\n",
    "        ,`email_address`\n",
    "        ,`full_name`\n",
    "        ,`email_address1`\n",
    "        ,`full_name1`\n",
    "    )\n",
    "    SELECT\n",
    "         `bg_source`.`bg_sourcesystem` AS `bg_sourcesystem`\n",
    "        ,`bg_source`.`bg_loadtimestamp` AS `bg_loadtimestamp`\n",
    "        ,1 AS `bg_errorcount`\n",
    "        ,'Nulled non-nullable business key \"full_name\"' AS `bg_errordescription`\n",
    "        ,`bg_source`.`bg_effectivetimestamp` AS `bg_effectivetimestamp`\n",
    "        ,`bg_source`.`bg_cleanse_id` AS `bg_cleanse_id`\n",
    "        ,`bg_source`.`customer_id` AS `customer_id`\n",
    "        ,`bg_source`.`email_address` AS `email_address`\n",
    "        ,`bg_source`.`full_name` AS `full_name`\n",
    "        ,`bg_source`.`email_address1` AS `email_address1`\n",
    "        ,`bg_source`.`full_name1` AS `full_name1`\n",
    "    FROM `{marc_databricks_initiative#cleanse#database_name}`.`{marc_databricks_initiative#cleanse#schema_name}`.`cls_en_ent_customers` AS `bg_source`\n",
    "    JOIN `{marc_databricks_initiative#cleanse#database_name}`.`{marc_databricks_initiative#cleanse#schema_name}`.`bg_erroneousrows_ent_customers_dataflow1` AS `bg_error`\n",
    "       ON `bg_source`.`bg_cleanse_id` = `bg_error`.`bg_cleanse_id`\n",
    "    \"\"\")\n",
    "    result_df = spark.sql(f\"\"\"\n",
    "    INSERT\n",
    "    INTO `{marc_databricks_initiative#cleanse#database_name}`.`{marc_databricks_initiative#cleanse#schema_name}`.`bg_excludedrows_ent_customers_dataflow1` (\n",
    "         `bg_cleanse_id`\n",
    "    )\n",
    "    SELECT\n",
    "         `bg_cleanse_id`\n",
    "    FROM `{marc_databricks_initiative#cleanse#database_name}`.`{marc_databricks_initiative#cleanse#schema_name}`.`bg_erroneousrows_ent_customers_dataflow1`\n",
    "    \"\"\")\n",
    "    RowCountInserted = result_df.select(\"num_inserted_rows\").collect()[0][0]\n",
    "    operation_metrics = result_df.toJSON().collect()\n",
    "    operation_metrics_collection['detectnonnullablenulledbusinesskeys_insertexcludedrows_{marc_databricks_initiative#cleanse#database_name}_{marc_databricks_initiative#cleanse#schema_name}_bg_excludedrows_ent_customers_dataflow1'] = operation_metrics\n",
    "    result_df = spark.sql(f\"\"\"\n",
    "    INSERT\n",
    "    OVERWRITE `{marc_databricks_initiative#cleanse#database_name}`.`{marc_databricks_initiative#cleanse#schema_name}`.`bg_erroneousrows_ent_customers_dataflow1` (\n",
    "         `bg_cleanse_id`\n",
    "    )\n",
    "    SELECT\n",
    "         `bg_source`.`bg_cleanse_id` AS `bg_cleanse_id`\n",
    "    FROM `{marc_databricks_initiative#cleanse#database_name}`.`{marc_databricks_initiative#cleanse#schema_name}`.`cls_en_ent_customers` AS `bg_source`\n",
    "    JOIN (\n",
    "        SELECT\n",
    "             `customer_id`\n",
    "            ,`email_address`\n",
    "            ,`full_name`\n",
    "        FROM `{marc_databricks_initiative#cleanse#database_name}`.`{marc_databricks_initiative#cleanse#schema_name}`.`cls_en_ent_customers`\n",
    "        GROUP BY\n",
    "             `customer_id`\n",
    "            ,`email_address`\n",
    "            ,`full_name`\n",
    "        HAVING COUNT(*) > 1\n",
    "    ) AS `bg_error`\n",
    "       ON ((`bg_source`.`customer_id` = `bg_error`.`customer_id`))\n",
    "      AND ((`bg_source`.`email_address` = `bg_error`.`email_address`))\n",
    "      AND ((`bg_source`.`full_name` = `bg_error`.`full_name`))\n",
    "    \"\"\")\n",
    "    result_df = spark.sql(f\"\"\"\n",
    "    INSERT\n",
    "    INTO `{marc_databricks_initiative#cleanse#database_name}`.`{marc_databricks_initiative#cleanse#schema_name}`.`cls_en_ent_customers_error` (\n",
    "         `bg_sourcesystem`\n",
    "        ,`bg_loadtimestamp`\n",
    "        ,`bg_errorcount`\n",
    "        ,`bg_errordescription`\n",
    "        ,`bg_effectivetimestamp`\n",
    "        ,`bg_cleanse_id`\n",
    "        ,`customer_id`\n",
    "        ,`email_address`\n",
    "        ,`full_name`\n",
    "        ,`email_address1`\n",
    "        ,`full_name1`\n",
    "    )\n",
    "    SELECT\n",
    "         `bg_source`.`bg_sourcesystem` AS `bg_sourcesystem`\n",
    "        ,`bg_source`.`bg_loadtimestamp` AS `bg_loadtimestamp`\n",
    "        ,1 AS `bg_errorcount`\n",
    "        ,'Duplicated business keys' AS `bg_errordescription`\n",
    "        ,`bg_source`.`bg_effectivetimestamp` AS `bg_effectivetimestamp`\n",
    "        ,`bg_source`.`bg_cleanse_id` AS `bg_cleanse_id`\n",
    "        ,`bg_source`.`customer_id` AS `customer_id`\n",
    "        ,`bg_source`.`email_address` AS `email_address`\n",
    "        ,`bg_source`.`full_name` AS `full_name`\n",
    "        ,`bg_source`.`email_address1` AS `email_address1`\n",
    "        ,`bg_source`.`full_name1` AS `full_name1`\n",
    "    FROM `{marc_databricks_initiative#cleanse#database_name}`.`{marc_databricks_initiative#cleanse#schema_name}`.`cls_en_ent_customers` AS `bg_source`\n",
    "    JOIN `{marc_databricks_initiative#cleanse#database_name}`.`{marc_databricks_initiative#cleanse#schema_name}`.`bg_erroneousrows_ent_customers_dataflow1` AS `bg_error`\n",
    "       ON `bg_source`.`bg_cleanse_id` = `bg_error`.`bg_cleanse_id`\n",
    "    \"\"\")\n",
    "    result_df = spark.sql(f\"\"\"\n",
    "    INSERT\n",
    "    INTO `{marc_databricks_initiative#cleanse#database_name}`.`{marc_databricks_initiative#cleanse#schema_name}`.`bg_excludedrows_ent_customers_dataflow1` (\n",
    "         `bg_cleanse_id`\n",
    "    )\n",
    "    SELECT\n",
    "         `bg_cleanse_id`\n",
    "    FROM `{marc_databricks_initiative#cleanse#database_name}`.`{marc_databricks_initiative#cleanse#schema_name}`.`bg_erroneousrows_ent_customers_dataflow1`\n",
    "    \"\"\")\n",
    "    RowCountInserted = result_df.select(\"num_inserted_rows\").collect()[0][0]\n",
    "    operation_metrics = result_df.toJSON().collect()\n",
    "    operation_metrics_collection['detectduplicatedbusinesskeys_insertexcludedrows_{marc_databricks_initiative#cleanse#database_name}_{marc_databricks_initiative#cleanse#schema_name}_bg_excludedrows_ent_customers_dataflow1'] = operation_metrics\n",
    "    result_df = spark.sql(f\"\"\"\n",
    "    MERGE\n",
    "    INTO `{marc_databricks_initiative#cleanse#database_name}`.`{marc_databricks_initiative#cleanse#schema_name}`.`cls_en_ent_customers` AS `bg_target`\n",
    "    USING (\n",
    "        SELECT\n",
    "             `bg_cleanse_id`\n",
    "            ,COUNT(*) AS `bg_errorcount`\n",
    "        FROM `{marc_databricks_initiative#cleanse#database_name}`.`{marc_databricks_initiative#cleanse#schema_name}`.`bg_excludedrows_ent_customers_dataflow1`\n",
    "        GROUP BY\n",
    "             `bg_cleanse_id`\n",
    "    ) AS `bg_error`\n",
    "       ON `bg_target`.`bg_cleanse_id` = `bg_error`.`bg_cleanse_id`\n",
    "    WHEN MATCHED\n",
    "    THEN\n",
    "        UPDATE \n",
    "        SET\n",
    "             `bg_errorcount` = `bg_error`.`bg_errorcount`\n",
    "    \"\"\")\n",
    "    RowCountInserted = result_df.select(\"num_inserted_rows\").collect()[0][0]\n",
    "    operation_metrics = result_df.toJSON().collect()\n",
    "    operation_metrics_collection['updateerrorcounts_{marc_databricks_initiative#cleanse#database_name}_{marc_databricks_initiative#cleanse#schema_name}_cls_en_ent_customers'] = operation_metrics\n",
    "\n",
    "except Exception as e:\n",
    "    info('CLS_EN_Ent_customers_Action', e)\n",
    "    raise\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3+lF9SY6",
   "metadata": {},
   "outputs": [],
   "source" : [
    "dbutils.notebook.exit(operation_metrics_collection)\n"
   ]
  } ],
 "metadata": {
  "kernelspec": {
   "display_name" : "Python 3 (ipykernel)",
   "language" : "python",
   "name" : "python3"
  },
  "language_info" : {
   "codemirror_mode" : {
    "name" : "ipython",
    "version" : 3
   },
   "file_extension" : ".py",
   "mimetype" : "text/x-python",
   "name" : "python",
   "nbconvert_exporter" : "python",
   "pygments_lexer" : "ipython3",
   "version" : "3.10.9"
  }
 },
 "nbformat" : 4,
 "nbformat_minor" : 5
}
