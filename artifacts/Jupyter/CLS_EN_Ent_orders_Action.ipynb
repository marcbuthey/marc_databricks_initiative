{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "n9txpoUp",
   "metadata": {},
   "outputs": [],
   "source" : [
    "# Setup input parameters\n",
    "from datetime import datetime as dt\n",
    "dbutils.widgets.text('bg_loadtimestamp', '')\n",
    "bg_loadtimestamp = dbutils.widgets.get('bg_loadtimestamp')\n",
    "bg_loadtimestamp_str = bg_loadtimestamp\n",
    "if not bg_loadtimestamp:\n",
    "    bg_loadtimestamp = 'CAST(NULL AS Timestamp)'\n",
    "else:\n",
    "    bg_loadtimestamp = f\"CAST('{bg_loadtimestamp}' AS Timestamp)\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "QcgXxQQY",
   "metadata": {},
   "outputs": [],
   "source" : [
    "# Setup logging\n",
    "logger = spark._jvm.org.apache.log4j.Logger.getLogger('com.bigenius-x.application')\n",
    "def info(targetName, message):\n",
    "    logger.info(f'{targetName}: {message}')\n",
    "    print(f\"{dt.now().strftime('%Y/%m/%d, %H:%M:%S')} - {targetName}: {message}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5gQFLb5",
   "metadata": {},
   "outputs": [],
   "source" : [
    "# EntityCleanseAction: Ent_orders_Entity Cleanse Action_1\n",
    "\n",
    "try:\n",
    "\n",
    "    operation_metrics_collection = {}\n",
    "    spark.sql(\"\"\"DROP TABLE IF EXISTS `{marc_databricks_initiative#cleanse#database_name}`.`{marc_databricks_initiative#cleanse#schema_name}`.`bg_erroneousrows_ent_orders_dataflow1`\"\"\")\n",
    "\n",
    "    spark.sql(\"\"\"CREATE TABLE `{marc_databricks_initiative#cleanse#database_name}`.`{marc_databricks_initiative#cleanse#schema_name}`.`bg_erroneousrows_ent_orders_dataflow1` (\n",
    "         `bg_cleanse_id` Long NOT NULL\n",
    "    )\n",
    "    USING delta\n",
    "    \"\"\"\n",
    "    )\n",
    "\n",
    "    spark.sql(\"\"\"DROP TABLE IF EXISTS `{marc_databricks_initiative#cleanse#database_name}`.`{marc_databricks_initiative#cleanse#schema_name}`.`bg_excludedrows_ent_orders_dataflow1`\"\"\")\n",
    "\n",
    "    spark.sql(\"\"\"CREATE TABLE `{marc_databricks_initiative#cleanse#database_name}`.`{marc_databricks_initiative#cleanse#schema_name}`.`bg_excludedrows_ent_orders_dataflow1` (\n",
    "         `bg_cleanse_id` Long NOT NULL\n",
    "    )\n",
    "    USING delta\n",
    "    \"\"\"\n",
    "    )\n",
    "\n",
    "    result_df = spark.sql(f\"\"\"\n",
    "    INSERT\n",
    "    OVERWRITE `{marc_databricks_initiative#cleanse#database_name}`.`{marc_databricks_initiative#cleanse#schema_name}`.`bg_erroneousrows_ent_orders_dataflow1` (\n",
    "         `bg_cleanse_id`\n",
    "    )\n",
    "    SELECT\n",
    "         `bg_cleanse_id`\n",
    "    FROM `{marc_databricks_initiative#cleanse#database_name}`.`{marc_databricks_initiative#cleanse#schema_name}`.`cls_en_ent_orders`\n",
    "    WHERE `order_id` IS NULL\n",
    "    \"\"\")\n",
    "    result_df = spark.sql(f\"\"\"\n",
    "    INSERT\n",
    "    INTO `{marc_databricks_initiative#cleanse#database_name}`.`{marc_databricks_initiative#cleanse#schema_name}`.`cls_en_ent_orders_error` (\n",
    "         `bg_sourcesystem`\n",
    "        ,`bg_loadtimestamp`\n",
    "        ,`bg_errorcount`\n",
    "        ,`bg_errordescription`\n",
    "        ,`bg_effectivetimestamp`\n",
    "        ,`bg_cleanse_id`\n",
    "        ,`order_id`\n",
    "        ,`order_status`\n",
    "        ,`order_total`\n",
    "    )\n",
    "    SELECT\n",
    "         `bg_source`.`bg_sourcesystem` AS `bg_sourcesystem`\n",
    "        ,`bg_source`.`bg_loadtimestamp` AS `bg_loadtimestamp`\n",
    "        ,1 AS `bg_errorcount`\n",
    "        ,'Nulled non-nullable business key \"order_id\"' AS `bg_errordescription`\n",
    "        ,`bg_source`.`bg_effectivetimestamp` AS `bg_effectivetimestamp`\n",
    "        ,`bg_source`.`bg_cleanse_id` AS `bg_cleanse_id`\n",
    "        ,`bg_source`.`order_id` AS `order_id`\n",
    "        ,`bg_source`.`order_status` AS `order_status`\n",
    "        ,`bg_source`.`order_total` AS `order_total`\n",
    "    FROM `{marc_databricks_initiative#cleanse#database_name}`.`{marc_databricks_initiative#cleanse#schema_name}`.`cls_en_ent_orders` AS `bg_source`\n",
    "    JOIN `{marc_databricks_initiative#cleanse#database_name}`.`{marc_databricks_initiative#cleanse#schema_name}`.`bg_erroneousrows_ent_orders_dataflow1` AS `bg_error`\n",
    "       ON `bg_source`.`bg_cleanse_id` = `bg_error`.`bg_cleanse_id`\n",
    "    \"\"\")\n",
    "    result_df = spark.sql(f\"\"\"\n",
    "    INSERT\n",
    "    INTO `{marc_databricks_initiative#cleanse#database_name}`.`{marc_databricks_initiative#cleanse#schema_name}`.`bg_excludedrows_ent_orders_dataflow1` (\n",
    "         `bg_cleanse_id`\n",
    "    )\n",
    "    SELECT\n",
    "         `bg_cleanse_id`\n",
    "    FROM `{marc_databricks_initiative#cleanse#database_name}`.`{marc_databricks_initiative#cleanse#schema_name}`.`bg_erroneousrows_ent_orders_dataflow1`\n",
    "    \"\"\")\n",
    "    RowCountInserted = result_df.select(\"num_inserted_rows\").collect()[0][0]\n",
    "    operation_metrics = result_df.toJSON().collect()\n",
    "    operation_metrics_collection['detectnonnullablenulledbusinesskeys_insertexcludedrows_{marc_databricks_initiative#cleanse#database_name}_{marc_databricks_initiative#cleanse#schema_name}_bg_excludedrows_ent_orders_dataflow1'] = operation_metrics\n",
    "    result_df = spark.sql(f\"\"\"\n",
    "    INSERT\n",
    "    OVERWRITE `{marc_databricks_initiative#cleanse#database_name}`.`{marc_databricks_initiative#cleanse#schema_name}`.`bg_erroneousrows_ent_orders_dataflow1` (\n",
    "         `bg_cleanse_id`\n",
    "    )\n",
    "    SELECT\n",
    "         `bg_cleanse_id`\n",
    "    FROM `{marc_databricks_initiative#cleanse#database_name}`.`{marc_databricks_initiative#cleanse#schema_name}`.`cls_en_ent_orders`\n",
    "    WHERE `order_status` IS NULL\n",
    "    \"\"\")\n",
    "    result_df = spark.sql(f\"\"\"\n",
    "    INSERT\n",
    "    INTO `{marc_databricks_initiative#cleanse#database_name}`.`{marc_databricks_initiative#cleanse#schema_name}`.`cls_en_ent_orders_error` (\n",
    "         `bg_sourcesystem`\n",
    "        ,`bg_loadtimestamp`\n",
    "        ,`bg_errorcount`\n",
    "        ,`bg_errordescription`\n",
    "        ,`bg_effectivetimestamp`\n",
    "        ,`bg_cleanse_id`\n",
    "        ,`order_id`\n",
    "        ,`order_status`\n",
    "        ,`order_total`\n",
    "    )\n",
    "    SELECT\n",
    "         `bg_source`.`bg_sourcesystem` AS `bg_sourcesystem`\n",
    "        ,`bg_source`.`bg_loadtimestamp` AS `bg_loadtimestamp`\n",
    "        ,1 AS `bg_errorcount`\n",
    "        ,'Nulled non-nullable business key \"order_status\"' AS `bg_errordescription`\n",
    "        ,`bg_source`.`bg_effectivetimestamp` AS `bg_effectivetimestamp`\n",
    "        ,`bg_source`.`bg_cleanse_id` AS `bg_cleanse_id`\n",
    "        ,`bg_source`.`order_id` AS `order_id`\n",
    "        ,`bg_source`.`order_status` AS `order_status`\n",
    "        ,`bg_source`.`order_total` AS `order_total`\n",
    "    FROM `{marc_databricks_initiative#cleanse#database_name}`.`{marc_databricks_initiative#cleanse#schema_name}`.`cls_en_ent_orders` AS `bg_source`\n",
    "    JOIN `{marc_databricks_initiative#cleanse#database_name}`.`{marc_databricks_initiative#cleanse#schema_name}`.`bg_erroneousrows_ent_orders_dataflow1` AS `bg_error`\n",
    "       ON `bg_source`.`bg_cleanse_id` = `bg_error`.`bg_cleanse_id`\n",
    "    \"\"\")\n",
    "    result_df = spark.sql(f\"\"\"\n",
    "    INSERT\n",
    "    INTO `{marc_databricks_initiative#cleanse#database_name}`.`{marc_databricks_initiative#cleanse#schema_name}`.`bg_excludedrows_ent_orders_dataflow1` (\n",
    "         `bg_cleanse_id`\n",
    "    )\n",
    "    SELECT\n",
    "         `bg_cleanse_id`\n",
    "    FROM `{marc_databricks_initiative#cleanse#database_name}`.`{marc_databricks_initiative#cleanse#schema_name}`.`bg_erroneousrows_ent_orders_dataflow1`\n",
    "    \"\"\")\n",
    "    RowCountInserted = result_df.select(\"num_inserted_rows\").collect()[0][0]\n",
    "    operation_metrics = result_df.toJSON().collect()\n",
    "    operation_metrics_collection['detectnonnullablenulledbusinesskeys_insertexcludedrows_{marc_databricks_initiative#cleanse#database_name}_{marc_databricks_initiative#cleanse#schema_name}_bg_excludedrows_ent_orders_dataflow1'] = operation_metrics\n",
    "    result_df = spark.sql(f\"\"\"\n",
    "    INSERT\n",
    "    OVERWRITE `{marc_databricks_initiative#cleanse#database_name}`.`{marc_databricks_initiative#cleanse#schema_name}`.`bg_erroneousrows_ent_orders_dataflow1` (\n",
    "         `bg_cleanse_id`\n",
    "    )\n",
    "    SELECT\n",
    "         `bg_cleanse_id`\n",
    "    FROM `{marc_databricks_initiative#cleanse#database_name}`.`{marc_databricks_initiative#cleanse#schema_name}`.`cls_en_ent_orders`\n",
    "    WHERE `order_total` IS NULL\n",
    "    \"\"\")\n",
    "    result_df = spark.sql(f\"\"\"\n",
    "    INSERT\n",
    "    INTO `{marc_databricks_initiative#cleanse#database_name}`.`{marc_databricks_initiative#cleanse#schema_name}`.`cls_en_ent_orders_error` (\n",
    "         `bg_sourcesystem`\n",
    "        ,`bg_loadtimestamp`\n",
    "        ,`bg_errorcount`\n",
    "        ,`bg_errordescription`\n",
    "        ,`bg_effectivetimestamp`\n",
    "        ,`bg_cleanse_id`\n",
    "        ,`order_id`\n",
    "        ,`order_status`\n",
    "        ,`order_total`\n",
    "    )\n",
    "    SELECT\n",
    "         `bg_source`.`bg_sourcesystem` AS `bg_sourcesystem`\n",
    "        ,`bg_source`.`bg_loadtimestamp` AS `bg_loadtimestamp`\n",
    "        ,1 AS `bg_errorcount`\n",
    "        ,'Nulled non-nullable business key \"order_total\"' AS `bg_errordescription`\n",
    "        ,`bg_source`.`bg_effectivetimestamp` AS `bg_effectivetimestamp`\n",
    "        ,`bg_source`.`bg_cleanse_id` AS `bg_cleanse_id`\n",
    "        ,`bg_source`.`order_id` AS `order_id`\n",
    "        ,`bg_source`.`order_status` AS `order_status`\n",
    "        ,`bg_source`.`order_total` AS `order_total`\n",
    "    FROM `{marc_databricks_initiative#cleanse#database_name}`.`{marc_databricks_initiative#cleanse#schema_name}`.`cls_en_ent_orders` AS `bg_source`\n",
    "    JOIN `{marc_databricks_initiative#cleanse#database_name}`.`{marc_databricks_initiative#cleanse#schema_name}`.`bg_erroneousrows_ent_orders_dataflow1` AS `bg_error`\n",
    "       ON `bg_source`.`bg_cleanse_id` = `bg_error`.`bg_cleanse_id`\n",
    "    \"\"\")\n",
    "    result_df = spark.sql(f\"\"\"\n",
    "    INSERT\n",
    "    INTO `{marc_databricks_initiative#cleanse#database_name}`.`{marc_databricks_initiative#cleanse#schema_name}`.`bg_excludedrows_ent_orders_dataflow1` (\n",
    "         `bg_cleanse_id`\n",
    "    )\n",
    "    SELECT\n",
    "         `bg_cleanse_id`\n",
    "    FROM `{marc_databricks_initiative#cleanse#database_name}`.`{marc_databricks_initiative#cleanse#schema_name}`.`bg_erroneousrows_ent_orders_dataflow1`\n",
    "    \"\"\")\n",
    "    RowCountInserted = result_df.select(\"num_inserted_rows\").collect()[0][0]\n",
    "    operation_metrics = result_df.toJSON().collect()\n",
    "    operation_metrics_collection['detectnonnullablenulledbusinesskeys_insertexcludedrows_{marc_databricks_initiative#cleanse#database_name}_{marc_databricks_initiative#cleanse#schema_name}_bg_excludedrows_ent_orders_dataflow1'] = operation_metrics\n",
    "    result_df = spark.sql(f\"\"\"\n",
    "    INSERT\n",
    "    OVERWRITE `{marc_databricks_initiative#cleanse#database_name}`.`{marc_databricks_initiative#cleanse#schema_name}`.`bg_erroneousrows_ent_orders_dataflow1` (\n",
    "         `bg_cleanse_id`\n",
    "    )\n",
    "    SELECT\n",
    "         `bg_source`.`bg_cleanse_id` AS `bg_cleanse_id`\n",
    "    FROM `{marc_databricks_initiative#cleanse#database_name}`.`{marc_databricks_initiative#cleanse#schema_name}`.`cls_en_ent_orders` AS `bg_source`\n",
    "    JOIN (\n",
    "        SELECT\n",
    "             `order_id`\n",
    "            ,`order_status`\n",
    "            ,`order_total`\n",
    "        FROM `{marc_databricks_initiative#cleanse#database_name}`.`{marc_databricks_initiative#cleanse#schema_name}`.`cls_en_ent_orders`\n",
    "        GROUP BY\n",
    "             `order_id`\n",
    "            ,`order_status`\n",
    "            ,`order_total`\n",
    "        HAVING COUNT(*) > 1\n",
    "    ) AS `bg_error`\n",
    "       ON ((`bg_source`.`order_id` = `bg_error`.`order_id`))\n",
    "      AND ((`bg_source`.`order_status` = `bg_error`.`order_status`))\n",
    "      AND ((`bg_source`.`order_total` = `bg_error`.`order_total`))\n",
    "    \"\"\")\n",
    "    result_df = spark.sql(f\"\"\"\n",
    "    INSERT\n",
    "    INTO `{marc_databricks_initiative#cleanse#database_name}`.`{marc_databricks_initiative#cleanse#schema_name}`.`cls_en_ent_orders_error` (\n",
    "         `bg_sourcesystem`\n",
    "        ,`bg_loadtimestamp`\n",
    "        ,`bg_errorcount`\n",
    "        ,`bg_errordescription`\n",
    "        ,`bg_effectivetimestamp`\n",
    "        ,`bg_cleanse_id`\n",
    "        ,`order_id`\n",
    "        ,`order_status`\n",
    "        ,`order_total`\n",
    "    )\n",
    "    SELECT\n",
    "         `bg_source`.`bg_sourcesystem` AS `bg_sourcesystem`\n",
    "        ,`bg_source`.`bg_loadtimestamp` AS `bg_loadtimestamp`\n",
    "        ,1 AS `bg_errorcount`\n",
    "        ,'Duplicated business keys' AS `bg_errordescription`\n",
    "        ,`bg_source`.`bg_effectivetimestamp` AS `bg_effectivetimestamp`\n",
    "        ,`bg_source`.`bg_cleanse_id` AS `bg_cleanse_id`\n",
    "        ,`bg_source`.`order_id` AS `order_id`\n",
    "        ,`bg_source`.`order_status` AS `order_status`\n",
    "        ,`bg_source`.`order_total` AS `order_total`\n",
    "    FROM `{marc_databricks_initiative#cleanse#database_name}`.`{marc_databricks_initiative#cleanse#schema_name}`.`cls_en_ent_orders` AS `bg_source`\n",
    "    JOIN `{marc_databricks_initiative#cleanse#database_name}`.`{marc_databricks_initiative#cleanse#schema_name}`.`bg_erroneousrows_ent_orders_dataflow1` AS `bg_error`\n",
    "       ON `bg_source`.`bg_cleanse_id` = `bg_error`.`bg_cleanse_id`\n",
    "    \"\"\")\n",
    "    result_df = spark.sql(f\"\"\"\n",
    "    INSERT\n",
    "    INTO `{marc_databricks_initiative#cleanse#database_name}`.`{marc_databricks_initiative#cleanse#schema_name}`.`bg_excludedrows_ent_orders_dataflow1` (\n",
    "         `bg_cleanse_id`\n",
    "    )\n",
    "    SELECT\n",
    "         `bg_cleanse_id`\n",
    "    FROM `{marc_databricks_initiative#cleanse#database_name}`.`{marc_databricks_initiative#cleanse#schema_name}`.`bg_erroneousrows_ent_orders_dataflow1`\n",
    "    \"\"\")\n",
    "    RowCountInserted = result_df.select(\"num_inserted_rows\").collect()[0][0]\n",
    "    operation_metrics = result_df.toJSON().collect()\n",
    "    operation_metrics_collection['detectduplicatedbusinesskeys_insertexcludedrows_{marc_databricks_initiative#cleanse#database_name}_{marc_databricks_initiative#cleanse#schema_name}_bg_excludedrows_ent_orders_dataflow1'] = operation_metrics\n",
    "    result_df = spark.sql(f\"\"\"\n",
    "    MERGE\n",
    "    INTO `{marc_databricks_initiative#cleanse#database_name}`.`{marc_databricks_initiative#cleanse#schema_name}`.`cls_en_ent_orders` AS `bg_target`\n",
    "    USING (\n",
    "        SELECT\n",
    "             `bg_cleanse_id`\n",
    "            ,COUNT(*) AS `bg_errorcount`\n",
    "        FROM `{marc_databricks_initiative#cleanse#database_name}`.`{marc_databricks_initiative#cleanse#schema_name}`.`bg_excludedrows_ent_orders_dataflow1`\n",
    "        GROUP BY\n",
    "             `bg_cleanse_id`\n",
    "    ) AS `bg_error`\n",
    "       ON `bg_target`.`bg_cleanse_id` = `bg_error`.`bg_cleanse_id`\n",
    "    WHEN MATCHED\n",
    "    THEN\n",
    "        UPDATE \n",
    "        SET\n",
    "             `bg_errorcount` = `bg_error`.`bg_errorcount`\n",
    "    \"\"\")\n",
    "    RowCountInserted = result_df.select(\"num_inserted_rows\").collect()[0][0]\n",
    "    operation_metrics = result_df.toJSON().collect()\n",
    "    operation_metrics_collection['updateerrorcounts_{marc_databricks_initiative#cleanse#database_name}_{marc_databricks_initiative#cleanse#schema_name}_cls_en_ent_orders'] = operation_metrics\n",
    "\n",
    "except Exception as e:\n",
    "    info('CLS_EN_Ent_orders_Action', e)\n",
    "    raise\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "L8iIYbli",
   "metadata": {},
   "outputs": [],
   "source" : [
    "dbutils.notebook.exit(operation_metrics_collection)\n"
   ]
  } ],
 "metadata": {
  "kernelspec": {
   "display_name" : "Python 3 (ipykernel)",
   "language" : "python",
   "name" : "python3"
  },
  "language_info" : {
   "codemirror_mode" : {
    "name" : "ipython",
    "version" : 3
   },
   "file_extension" : ".py",
   "mimetype" : "text/x-python",
   "name" : "python",
   "nbconvert_exporter" : "python",
   "pygments_lexer" : "ipython3",
   "version" : "3.10.9"
  }
 },
 "nbformat" : 4,
 "nbformat_minor" : 5
}
