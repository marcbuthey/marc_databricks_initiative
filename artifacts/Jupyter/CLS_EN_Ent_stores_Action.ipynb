{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Dy6dzKhq",
   "metadata": {},
   "outputs": [],
   "source" : [
    "# Setup input parameters\n",
    "from datetime import datetime as dt\n",
    "dbutils.widgets.text('bg_loadtimestamp', '')\n",
    "bg_loadtimestamp = dbutils.widgets.get('bg_loadtimestamp')\n",
    "bg_loadtimestamp_str = bg_loadtimestamp\n",
    "if not bg_loadtimestamp:\n",
    "    bg_loadtimestamp = 'CAST(NULL AS Timestamp)'\n",
    "else:\n",
    "    bg_loadtimestamp = f\"CAST('{bg_loadtimestamp}' AS Timestamp)\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DJgxL8nW",
   "metadata": {},
   "outputs": [],
   "source" : [
    "# Setup logging\n",
    "logger = spark._jvm.org.apache.log4j.Logger.getLogger('com.bigenius-x.application')\n",
    "def info(targetName, message):\n",
    "    logger.info(f'{targetName}: {message}')\n",
    "    print(f\"{dt.now().strftime('%Y/%m/%d, %H:%M:%S')} - {targetName}: {message}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MoYp3jq2",
   "metadata": {},
   "outputs": [],
   "source" : [
    "# EntityCleanseAction: Ent_stores_Entity Cleanse Action_1\n",
    "\n",
    "try:\n",
    "\n",
    "    operation_metrics_collection = {}\n",
    "    spark.sql(\"\"\"DROP TABLE IF EXISTS `{marc_databricks_initiative#cleanse#database_name}`.`{marc_databricks_initiative#cleanse#schema_name}`.`bg_erroneousrows_ent_stores_dataflow1`\"\"\")\n",
    "\n",
    "    spark.sql(\"\"\"CREATE TABLE `{marc_databricks_initiative#cleanse#database_name}`.`{marc_databricks_initiative#cleanse#schema_name}`.`bg_erroneousrows_ent_stores_dataflow1` (\n",
    "         `bg_cleanse_id` Long NOT NULL\n",
    "    )\n",
    "    USING delta\n",
    "    \"\"\"\n",
    "    )\n",
    "\n",
    "    spark.sql(\"\"\"DROP TABLE IF EXISTS `{marc_databricks_initiative#cleanse#database_name}`.`{marc_databricks_initiative#cleanse#schema_name}`.`bg_excludedrows_ent_stores_dataflow1`\"\"\")\n",
    "\n",
    "    spark.sql(\"\"\"CREATE TABLE `{marc_databricks_initiative#cleanse#database_name}`.`{marc_databricks_initiative#cleanse#schema_name}`.`bg_excludedrows_ent_stores_dataflow1` (\n",
    "         `bg_cleanse_id` Long NOT NULL\n",
    "    )\n",
    "    USING delta\n",
    "    \"\"\"\n",
    "    )\n",
    "\n",
    "    result_df = spark.sql(f\"\"\"\n",
    "    INSERT\n",
    "    OVERWRITE `{marc_databricks_initiative#cleanse#database_name}`.`{marc_databricks_initiative#cleanse#schema_name}`.`bg_erroneousrows_ent_stores_dataflow1` (\n",
    "         `bg_cleanse_id`\n",
    "    )\n",
    "    SELECT\n",
    "         `bg_cleanse_id`\n",
    "    FROM `{marc_databricks_initiative#cleanse#database_name}`.`{marc_databricks_initiative#cleanse#schema_name}`.`cls_en_ent_stores`\n",
    "    WHERE `store_name` IS NULL\n",
    "    \"\"\")\n",
    "    result_df = spark.sql(f\"\"\"\n",
    "    INSERT\n",
    "    INTO `{marc_databricks_initiative#cleanse#database_name}`.`{marc_databricks_initiative#cleanse#schema_name}`.`cls_en_ent_stores_error` (\n",
    "         `bg_sourcesystem`\n",
    "        ,`bg_loadtimestamp`\n",
    "        ,`bg_errorcount`\n",
    "        ,`bg_errordescription`\n",
    "        ,`bg_effectivetimestamp`\n",
    "        ,`bg_cleanse_id`\n",
    "        ,`store_name`\n",
    "        ,`web_address`\n",
    "        ,`latitude`\n",
    "        ,`longitude`\n",
    "        ,`physical_address`\n",
    "        ,`store_id`\n",
    "    )\n",
    "    SELECT\n",
    "         `bg_source`.`bg_sourcesystem` AS `bg_sourcesystem`\n",
    "        ,`bg_source`.`bg_loadtimestamp` AS `bg_loadtimestamp`\n",
    "        ,1 AS `bg_errorcount`\n",
    "        ,'Nulled non-nullable business key \"store_name\"' AS `bg_errordescription`\n",
    "        ,`bg_source`.`bg_effectivetimestamp` AS `bg_effectivetimestamp`\n",
    "        ,`bg_source`.`bg_cleanse_id` AS `bg_cleanse_id`\n",
    "        ,`bg_source`.`store_name` AS `store_name`\n",
    "        ,`bg_source`.`web_address` AS `web_address`\n",
    "        ,`bg_source`.`latitude` AS `latitude`\n",
    "        ,`bg_source`.`longitude` AS `longitude`\n",
    "        ,`bg_source`.`physical_address` AS `physical_address`\n",
    "        ,`bg_source`.`store_id` AS `store_id`\n",
    "    FROM `{marc_databricks_initiative#cleanse#database_name}`.`{marc_databricks_initiative#cleanse#schema_name}`.`cls_en_ent_stores` AS `bg_source`\n",
    "    JOIN `{marc_databricks_initiative#cleanse#database_name}`.`{marc_databricks_initiative#cleanse#schema_name}`.`bg_erroneousrows_ent_stores_dataflow1` AS `bg_error`\n",
    "       ON `bg_source`.`bg_cleanse_id` = `bg_error`.`bg_cleanse_id`\n",
    "    \"\"\")\n",
    "    result_df = spark.sql(f\"\"\"\n",
    "    INSERT\n",
    "    INTO `{marc_databricks_initiative#cleanse#database_name}`.`{marc_databricks_initiative#cleanse#schema_name}`.`bg_excludedrows_ent_stores_dataflow1` (\n",
    "         `bg_cleanse_id`\n",
    "    )\n",
    "    SELECT\n",
    "         `bg_cleanse_id`\n",
    "    FROM `{marc_databricks_initiative#cleanse#database_name}`.`{marc_databricks_initiative#cleanse#schema_name}`.`bg_erroneousrows_ent_stores_dataflow1`\n",
    "    \"\"\")\n",
    "    RowCountInserted = result_df.select(\"num_inserted_rows\").collect()[0][0]\n",
    "    operation_metrics = result_df.toJSON().collect()\n",
    "    operation_metrics_collection['detectnonnullablenulledbusinesskeys_insertexcludedrows_{marc_databricks_initiative#cleanse#database_name}_{marc_databricks_initiative#cleanse#schema_name}_bg_excludedrows_ent_stores_dataflow1'] = operation_metrics\n",
    "    result_df = spark.sql(f\"\"\"\n",
    "    INSERT\n",
    "    OVERWRITE `{marc_databricks_initiative#cleanse#database_name}`.`{marc_databricks_initiative#cleanse#schema_name}`.`bg_erroneousrows_ent_stores_dataflow1` (\n",
    "         `bg_cleanse_id`\n",
    "    )\n",
    "    SELECT\n",
    "         `bg_cleanse_id`\n",
    "    FROM `{marc_databricks_initiative#cleanse#database_name}`.`{marc_databricks_initiative#cleanse#schema_name}`.`cls_en_ent_stores`\n",
    "    WHERE `web_address` IS NULL\n",
    "    \"\"\")\n",
    "    result_df = spark.sql(f\"\"\"\n",
    "    INSERT\n",
    "    INTO `{marc_databricks_initiative#cleanse#database_name}`.`{marc_databricks_initiative#cleanse#schema_name}`.`cls_en_ent_stores_error` (\n",
    "         `bg_sourcesystem`\n",
    "        ,`bg_loadtimestamp`\n",
    "        ,`bg_errorcount`\n",
    "        ,`bg_errordescription`\n",
    "        ,`bg_effectivetimestamp`\n",
    "        ,`bg_cleanse_id`\n",
    "        ,`store_name`\n",
    "        ,`web_address`\n",
    "        ,`latitude`\n",
    "        ,`longitude`\n",
    "        ,`physical_address`\n",
    "        ,`store_id`\n",
    "    )\n",
    "    SELECT\n",
    "         `bg_source`.`bg_sourcesystem` AS `bg_sourcesystem`\n",
    "        ,`bg_source`.`bg_loadtimestamp` AS `bg_loadtimestamp`\n",
    "        ,1 AS `bg_errorcount`\n",
    "        ,'Nulled non-nullable business key \"web_address\"' AS `bg_errordescription`\n",
    "        ,`bg_source`.`bg_effectivetimestamp` AS `bg_effectivetimestamp`\n",
    "        ,`bg_source`.`bg_cleanse_id` AS `bg_cleanse_id`\n",
    "        ,`bg_source`.`store_name` AS `store_name`\n",
    "        ,`bg_source`.`web_address` AS `web_address`\n",
    "        ,`bg_source`.`latitude` AS `latitude`\n",
    "        ,`bg_source`.`longitude` AS `longitude`\n",
    "        ,`bg_source`.`physical_address` AS `physical_address`\n",
    "        ,`bg_source`.`store_id` AS `store_id`\n",
    "    FROM `{marc_databricks_initiative#cleanse#database_name}`.`{marc_databricks_initiative#cleanse#schema_name}`.`cls_en_ent_stores` AS `bg_source`\n",
    "    JOIN `{marc_databricks_initiative#cleanse#database_name}`.`{marc_databricks_initiative#cleanse#schema_name}`.`bg_erroneousrows_ent_stores_dataflow1` AS `bg_error`\n",
    "       ON `bg_source`.`bg_cleanse_id` = `bg_error`.`bg_cleanse_id`\n",
    "    \"\"\")\n",
    "    result_df = spark.sql(f\"\"\"\n",
    "    INSERT\n",
    "    INTO `{marc_databricks_initiative#cleanse#database_name}`.`{marc_databricks_initiative#cleanse#schema_name}`.`bg_excludedrows_ent_stores_dataflow1` (\n",
    "         `bg_cleanse_id`\n",
    "    )\n",
    "    SELECT\n",
    "         `bg_cleanse_id`\n",
    "    FROM `{marc_databricks_initiative#cleanse#database_name}`.`{marc_databricks_initiative#cleanse#schema_name}`.`bg_erroneousrows_ent_stores_dataflow1`\n",
    "    \"\"\")\n",
    "    RowCountInserted = result_df.select(\"num_inserted_rows\").collect()[0][0]\n",
    "    operation_metrics = result_df.toJSON().collect()\n",
    "    operation_metrics_collection['detectnonnullablenulledbusinesskeys_insertexcludedrows_{marc_databricks_initiative#cleanse#database_name}_{marc_databricks_initiative#cleanse#schema_name}_bg_excludedrows_ent_stores_dataflow1'] = operation_metrics\n",
    "    result_df = spark.sql(f\"\"\"\n",
    "    INSERT\n",
    "    OVERWRITE `{marc_databricks_initiative#cleanse#database_name}`.`{marc_databricks_initiative#cleanse#schema_name}`.`bg_erroneousrows_ent_stores_dataflow1` (\n",
    "         `bg_cleanse_id`\n",
    "    )\n",
    "    SELECT\n",
    "         `bg_cleanse_id`\n",
    "    FROM `{marc_databricks_initiative#cleanse#database_name}`.`{marc_databricks_initiative#cleanse#schema_name}`.`cls_en_ent_stores`\n",
    "    WHERE `latitude` IS NULL\n",
    "    \"\"\")\n",
    "    result_df = spark.sql(f\"\"\"\n",
    "    INSERT\n",
    "    INTO `{marc_databricks_initiative#cleanse#database_name}`.`{marc_databricks_initiative#cleanse#schema_name}`.`cls_en_ent_stores_error` (\n",
    "         `bg_sourcesystem`\n",
    "        ,`bg_loadtimestamp`\n",
    "        ,`bg_errorcount`\n",
    "        ,`bg_errordescription`\n",
    "        ,`bg_effectivetimestamp`\n",
    "        ,`bg_cleanse_id`\n",
    "        ,`store_name`\n",
    "        ,`web_address`\n",
    "        ,`latitude`\n",
    "        ,`longitude`\n",
    "        ,`physical_address`\n",
    "        ,`store_id`\n",
    "    )\n",
    "    SELECT\n",
    "         `bg_source`.`bg_sourcesystem` AS `bg_sourcesystem`\n",
    "        ,`bg_source`.`bg_loadtimestamp` AS `bg_loadtimestamp`\n",
    "        ,1 AS `bg_errorcount`\n",
    "        ,'Nulled non-nullable business key \"latitude\"' AS `bg_errordescription`\n",
    "        ,`bg_source`.`bg_effectivetimestamp` AS `bg_effectivetimestamp`\n",
    "        ,`bg_source`.`bg_cleanse_id` AS `bg_cleanse_id`\n",
    "        ,`bg_source`.`store_name` AS `store_name`\n",
    "        ,`bg_source`.`web_address` AS `web_address`\n",
    "        ,`bg_source`.`latitude` AS `latitude`\n",
    "        ,`bg_source`.`longitude` AS `longitude`\n",
    "        ,`bg_source`.`physical_address` AS `physical_address`\n",
    "        ,`bg_source`.`store_id` AS `store_id`\n",
    "    FROM `{marc_databricks_initiative#cleanse#database_name}`.`{marc_databricks_initiative#cleanse#schema_name}`.`cls_en_ent_stores` AS `bg_source`\n",
    "    JOIN `{marc_databricks_initiative#cleanse#database_name}`.`{marc_databricks_initiative#cleanse#schema_name}`.`bg_erroneousrows_ent_stores_dataflow1` AS `bg_error`\n",
    "       ON `bg_source`.`bg_cleanse_id` = `bg_error`.`bg_cleanse_id`\n",
    "    \"\"\")\n",
    "    result_df = spark.sql(f\"\"\"\n",
    "    INSERT\n",
    "    INTO `{marc_databricks_initiative#cleanse#database_name}`.`{marc_databricks_initiative#cleanse#schema_name}`.`bg_excludedrows_ent_stores_dataflow1` (\n",
    "         `bg_cleanse_id`\n",
    "    )\n",
    "    SELECT\n",
    "         `bg_cleanse_id`\n",
    "    FROM `{marc_databricks_initiative#cleanse#database_name}`.`{marc_databricks_initiative#cleanse#schema_name}`.`bg_erroneousrows_ent_stores_dataflow1`\n",
    "    \"\"\")\n",
    "    RowCountInserted = result_df.select(\"num_inserted_rows\").collect()[0][0]\n",
    "    operation_metrics = result_df.toJSON().collect()\n",
    "    operation_metrics_collection['detectnonnullablenulledbusinesskeys_insertexcludedrows_{marc_databricks_initiative#cleanse#database_name}_{marc_databricks_initiative#cleanse#schema_name}_bg_excludedrows_ent_stores_dataflow1'] = operation_metrics\n",
    "    result_df = spark.sql(f\"\"\"\n",
    "    INSERT\n",
    "    OVERWRITE `{marc_databricks_initiative#cleanse#database_name}`.`{marc_databricks_initiative#cleanse#schema_name}`.`bg_erroneousrows_ent_stores_dataflow1` (\n",
    "         `bg_cleanse_id`\n",
    "    )\n",
    "    SELECT\n",
    "         `bg_cleanse_id`\n",
    "    FROM `{marc_databricks_initiative#cleanse#database_name}`.`{marc_databricks_initiative#cleanse#schema_name}`.`cls_en_ent_stores`\n",
    "    WHERE `longitude` IS NULL\n",
    "    \"\"\")\n",
    "    result_df = spark.sql(f\"\"\"\n",
    "    INSERT\n",
    "    INTO `{marc_databricks_initiative#cleanse#database_name}`.`{marc_databricks_initiative#cleanse#schema_name}`.`cls_en_ent_stores_error` (\n",
    "         `bg_sourcesystem`\n",
    "        ,`bg_loadtimestamp`\n",
    "        ,`bg_errorcount`\n",
    "        ,`bg_errordescription`\n",
    "        ,`bg_effectivetimestamp`\n",
    "        ,`bg_cleanse_id`\n",
    "        ,`store_name`\n",
    "        ,`web_address`\n",
    "        ,`latitude`\n",
    "        ,`longitude`\n",
    "        ,`physical_address`\n",
    "        ,`store_id`\n",
    "    )\n",
    "    SELECT\n",
    "         `bg_source`.`bg_sourcesystem` AS `bg_sourcesystem`\n",
    "        ,`bg_source`.`bg_loadtimestamp` AS `bg_loadtimestamp`\n",
    "        ,1 AS `bg_errorcount`\n",
    "        ,'Nulled non-nullable business key \"longitude\"' AS `bg_errordescription`\n",
    "        ,`bg_source`.`bg_effectivetimestamp` AS `bg_effectivetimestamp`\n",
    "        ,`bg_source`.`bg_cleanse_id` AS `bg_cleanse_id`\n",
    "        ,`bg_source`.`store_name` AS `store_name`\n",
    "        ,`bg_source`.`web_address` AS `web_address`\n",
    "        ,`bg_source`.`latitude` AS `latitude`\n",
    "        ,`bg_source`.`longitude` AS `longitude`\n",
    "        ,`bg_source`.`physical_address` AS `physical_address`\n",
    "        ,`bg_source`.`store_id` AS `store_id`\n",
    "    FROM `{marc_databricks_initiative#cleanse#database_name}`.`{marc_databricks_initiative#cleanse#schema_name}`.`cls_en_ent_stores` AS `bg_source`\n",
    "    JOIN `{marc_databricks_initiative#cleanse#database_name}`.`{marc_databricks_initiative#cleanse#schema_name}`.`bg_erroneousrows_ent_stores_dataflow1` AS `bg_error`\n",
    "       ON `bg_source`.`bg_cleanse_id` = `bg_error`.`bg_cleanse_id`\n",
    "    \"\"\")\n",
    "    result_df = spark.sql(f\"\"\"\n",
    "    INSERT\n",
    "    INTO `{marc_databricks_initiative#cleanse#database_name}`.`{marc_databricks_initiative#cleanse#schema_name}`.`bg_excludedrows_ent_stores_dataflow1` (\n",
    "         `bg_cleanse_id`\n",
    "    )\n",
    "    SELECT\n",
    "         `bg_cleanse_id`\n",
    "    FROM `{marc_databricks_initiative#cleanse#database_name}`.`{marc_databricks_initiative#cleanse#schema_name}`.`bg_erroneousrows_ent_stores_dataflow1`\n",
    "    \"\"\")\n",
    "    RowCountInserted = result_df.select(\"num_inserted_rows\").collect()[0][0]\n",
    "    operation_metrics = result_df.toJSON().collect()\n",
    "    operation_metrics_collection['detectnonnullablenulledbusinesskeys_insertexcludedrows_{marc_databricks_initiative#cleanse#database_name}_{marc_databricks_initiative#cleanse#schema_name}_bg_excludedrows_ent_stores_dataflow1'] = operation_metrics\n",
    "    result_df = spark.sql(f\"\"\"\n",
    "    INSERT\n",
    "    OVERWRITE `{marc_databricks_initiative#cleanse#database_name}`.`{marc_databricks_initiative#cleanse#schema_name}`.`bg_erroneousrows_ent_stores_dataflow1` (\n",
    "         `bg_cleanse_id`\n",
    "    )\n",
    "    SELECT\n",
    "         `bg_cleanse_id`\n",
    "    FROM `{marc_databricks_initiative#cleanse#database_name}`.`{marc_databricks_initiative#cleanse#schema_name}`.`cls_en_ent_stores`\n",
    "    WHERE `physical_address` IS NULL\n",
    "    \"\"\")\n",
    "    result_df = spark.sql(f\"\"\"\n",
    "    INSERT\n",
    "    INTO `{marc_databricks_initiative#cleanse#database_name}`.`{marc_databricks_initiative#cleanse#schema_name}`.`cls_en_ent_stores_error` (\n",
    "         `bg_sourcesystem`\n",
    "        ,`bg_loadtimestamp`\n",
    "        ,`bg_errorcount`\n",
    "        ,`bg_errordescription`\n",
    "        ,`bg_effectivetimestamp`\n",
    "        ,`bg_cleanse_id`\n",
    "        ,`store_name`\n",
    "        ,`web_address`\n",
    "        ,`latitude`\n",
    "        ,`longitude`\n",
    "        ,`physical_address`\n",
    "        ,`store_id`\n",
    "    )\n",
    "    SELECT\n",
    "         `bg_source`.`bg_sourcesystem` AS `bg_sourcesystem`\n",
    "        ,`bg_source`.`bg_loadtimestamp` AS `bg_loadtimestamp`\n",
    "        ,1 AS `bg_errorcount`\n",
    "        ,'Nulled non-nullable business key \"physical_address\"' AS `bg_errordescription`\n",
    "        ,`bg_source`.`bg_effectivetimestamp` AS `bg_effectivetimestamp`\n",
    "        ,`bg_source`.`bg_cleanse_id` AS `bg_cleanse_id`\n",
    "        ,`bg_source`.`store_name` AS `store_name`\n",
    "        ,`bg_source`.`web_address` AS `web_address`\n",
    "        ,`bg_source`.`latitude` AS `latitude`\n",
    "        ,`bg_source`.`longitude` AS `longitude`\n",
    "        ,`bg_source`.`physical_address` AS `physical_address`\n",
    "        ,`bg_source`.`store_id` AS `store_id`\n",
    "    FROM `{marc_databricks_initiative#cleanse#database_name}`.`{marc_databricks_initiative#cleanse#schema_name}`.`cls_en_ent_stores` AS `bg_source`\n",
    "    JOIN `{marc_databricks_initiative#cleanse#database_name}`.`{marc_databricks_initiative#cleanse#schema_name}`.`bg_erroneousrows_ent_stores_dataflow1` AS `bg_error`\n",
    "       ON `bg_source`.`bg_cleanse_id` = `bg_error`.`bg_cleanse_id`\n",
    "    \"\"\")\n",
    "    result_df = spark.sql(f\"\"\"\n",
    "    INSERT\n",
    "    INTO `{marc_databricks_initiative#cleanse#database_name}`.`{marc_databricks_initiative#cleanse#schema_name}`.`bg_excludedrows_ent_stores_dataflow1` (\n",
    "         `bg_cleanse_id`\n",
    "    )\n",
    "    SELECT\n",
    "         `bg_cleanse_id`\n",
    "    FROM `{marc_databricks_initiative#cleanse#database_name}`.`{marc_databricks_initiative#cleanse#schema_name}`.`bg_erroneousrows_ent_stores_dataflow1`\n",
    "    \"\"\")\n",
    "    RowCountInserted = result_df.select(\"num_inserted_rows\").collect()[0][0]\n",
    "    operation_metrics = result_df.toJSON().collect()\n",
    "    operation_metrics_collection['detectnonnullablenulledbusinesskeys_insertexcludedrows_{marc_databricks_initiative#cleanse#database_name}_{marc_databricks_initiative#cleanse#schema_name}_bg_excludedrows_ent_stores_dataflow1'] = operation_metrics\n",
    "    result_df = spark.sql(f\"\"\"\n",
    "    INSERT\n",
    "    OVERWRITE `{marc_databricks_initiative#cleanse#database_name}`.`{marc_databricks_initiative#cleanse#schema_name}`.`bg_erroneousrows_ent_stores_dataflow1` (\n",
    "         `bg_cleanse_id`\n",
    "    )\n",
    "    SELECT\n",
    "         `bg_cleanse_id`\n",
    "    FROM `{marc_databricks_initiative#cleanse#database_name}`.`{marc_databricks_initiative#cleanse#schema_name}`.`cls_en_ent_stores`\n",
    "    WHERE `store_id` IS NULL\n",
    "    \"\"\")\n",
    "    result_df = spark.sql(f\"\"\"\n",
    "    INSERT\n",
    "    INTO `{marc_databricks_initiative#cleanse#database_name}`.`{marc_databricks_initiative#cleanse#schema_name}`.`cls_en_ent_stores_error` (\n",
    "         `bg_sourcesystem`\n",
    "        ,`bg_loadtimestamp`\n",
    "        ,`bg_errorcount`\n",
    "        ,`bg_errordescription`\n",
    "        ,`bg_effectivetimestamp`\n",
    "        ,`bg_cleanse_id`\n",
    "        ,`store_name`\n",
    "        ,`web_address`\n",
    "        ,`latitude`\n",
    "        ,`longitude`\n",
    "        ,`physical_address`\n",
    "        ,`store_id`\n",
    "    )\n",
    "    SELECT\n",
    "         `bg_source`.`bg_sourcesystem` AS `bg_sourcesystem`\n",
    "        ,`bg_source`.`bg_loadtimestamp` AS `bg_loadtimestamp`\n",
    "        ,1 AS `bg_errorcount`\n",
    "        ,'Nulled non-nullable business key \"store_id\"' AS `bg_errordescription`\n",
    "        ,`bg_source`.`bg_effectivetimestamp` AS `bg_effectivetimestamp`\n",
    "        ,`bg_source`.`bg_cleanse_id` AS `bg_cleanse_id`\n",
    "        ,`bg_source`.`store_name` AS `store_name`\n",
    "        ,`bg_source`.`web_address` AS `web_address`\n",
    "        ,`bg_source`.`latitude` AS `latitude`\n",
    "        ,`bg_source`.`longitude` AS `longitude`\n",
    "        ,`bg_source`.`physical_address` AS `physical_address`\n",
    "        ,`bg_source`.`store_id` AS `store_id`\n",
    "    FROM `{marc_databricks_initiative#cleanse#database_name}`.`{marc_databricks_initiative#cleanse#schema_name}`.`cls_en_ent_stores` AS `bg_source`\n",
    "    JOIN `{marc_databricks_initiative#cleanse#database_name}`.`{marc_databricks_initiative#cleanse#schema_name}`.`bg_erroneousrows_ent_stores_dataflow1` AS `bg_error`\n",
    "       ON `bg_source`.`bg_cleanse_id` = `bg_error`.`bg_cleanse_id`\n",
    "    \"\"\")\n",
    "    result_df = spark.sql(f\"\"\"\n",
    "    INSERT\n",
    "    INTO `{marc_databricks_initiative#cleanse#database_name}`.`{marc_databricks_initiative#cleanse#schema_name}`.`bg_excludedrows_ent_stores_dataflow1` (\n",
    "         `bg_cleanse_id`\n",
    "    )\n",
    "    SELECT\n",
    "         `bg_cleanse_id`\n",
    "    FROM `{marc_databricks_initiative#cleanse#database_name}`.`{marc_databricks_initiative#cleanse#schema_name}`.`bg_erroneousrows_ent_stores_dataflow1`\n",
    "    \"\"\")\n",
    "    RowCountInserted = result_df.select(\"num_inserted_rows\").collect()[0][0]\n",
    "    operation_metrics = result_df.toJSON().collect()\n",
    "    operation_metrics_collection['detectnonnullablenulledbusinesskeys_insertexcludedrows_{marc_databricks_initiative#cleanse#database_name}_{marc_databricks_initiative#cleanse#schema_name}_bg_excludedrows_ent_stores_dataflow1'] = operation_metrics\n",
    "    result_df = spark.sql(f\"\"\"\n",
    "    INSERT\n",
    "    OVERWRITE `{marc_databricks_initiative#cleanse#database_name}`.`{marc_databricks_initiative#cleanse#schema_name}`.`bg_erroneousrows_ent_stores_dataflow1` (\n",
    "         `bg_cleanse_id`\n",
    "    )\n",
    "    SELECT\n",
    "         `bg_source`.`bg_cleanse_id` AS `bg_cleanse_id`\n",
    "    FROM `{marc_databricks_initiative#cleanse#database_name}`.`{marc_databricks_initiative#cleanse#schema_name}`.`cls_en_ent_stores` AS `bg_source`\n",
    "    JOIN (\n",
    "        SELECT\n",
    "             `store_name`\n",
    "            ,`web_address`\n",
    "            ,`latitude`\n",
    "            ,`longitude`\n",
    "            ,`physical_address`\n",
    "            ,`store_id`\n",
    "        FROM `{marc_databricks_initiative#cleanse#database_name}`.`{marc_databricks_initiative#cleanse#schema_name}`.`cls_en_ent_stores`\n",
    "        GROUP BY\n",
    "             `store_name`\n",
    "            ,`web_address`\n",
    "            ,`latitude`\n",
    "            ,`longitude`\n",
    "            ,`physical_address`\n",
    "            ,`store_id`\n",
    "        HAVING COUNT(*) > 1\n",
    "    ) AS `bg_error`\n",
    "       ON ((`bg_source`.`store_name` = `bg_error`.`store_name`))\n",
    "      AND ((`bg_source`.`web_address` = `bg_error`.`web_address`))\n",
    "      AND ((`bg_source`.`latitude` = `bg_error`.`latitude`))\n",
    "      AND ((`bg_source`.`longitude` = `bg_error`.`longitude`))\n",
    "      AND ((`bg_source`.`physical_address` = `bg_error`.`physical_address`))\n",
    "      AND ((`bg_source`.`store_id` = `bg_error`.`store_id`))\n",
    "    \"\"\")\n",
    "    result_df = spark.sql(f\"\"\"\n",
    "    INSERT\n",
    "    INTO `{marc_databricks_initiative#cleanse#database_name}`.`{marc_databricks_initiative#cleanse#schema_name}`.`cls_en_ent_stores_error` (\n",
    "         `bg_sourcesystem`\n",
    "        ,`bg_loadtimestamp`\n",
    "        ,`bg_errorcount`\n",
    "        ,`bg_errordescription`\n",
    "        ,`bg_effectivetimestamp`\n",
    "        ,`bg_cleanse_id`\n",
    "        ,`store_name`\n",
    "        ,`web_address`\n",
    "        ,`latitude`\n",
    "        ,`longitude`\n",
    "        ,`physical_address`\n",
    "        ,`store_id`\n",
    "    )\n",
    "    SELECT\n",
    "         `bg_source`.`bg_sourcesystem` AS `bg_sourcesystem`\n",
    "        ,`bg_source`.`bg_loadtimestamp` AS `bg_loadtimestamp`\n",
    "        ,1 AS `bg_errorcount`\n",
    "        ,'Duplicated business keys' AS `bg_errordescription`\n",
    "        ,`bg_source`.`bg_effectivetimestamp` AS `bg_effectivetimestamp`\n",
    "        ,`bg_source`.`bg_cleanse_id` AS `bg_cleanse_id`\n",
    "        ,`bg_source`.`store_name` AS `store_name`\n",
    "        ,`bg_source`.`web_address` AS `web_address`\n",
    "        ,`bg_source`.`latitude` AS `latitude`\n",
    "        ,`bg_source`.`longitude` AS `longitude`\n",
    "        ,`bg_source`.`physical_address` AS `physical_address`\n",
    "        ,`bg_source`.`store_id` AS `store_id`\n",
    "    FROM `{marc_databricks_initiative#cleanse#database_name}`.`{marc_databricks_initiative#cleanse#schema_name}`.`cls_en_ent_stores` AS `bg_source`\n",
    "    JOIN `{marc_databricks_initiative#cleanse#database_name}`.`{marc_databricks_initiative#cleanse#schema_name}`.`bg_erroneousrows_ent_stores_dataflow1` AS `bg_error`\n",
    "       ON `bg_source`.`bg_cleanse_id` = `bg_error`.`bg_cleanse_id`\n",
    "    \"\"\")\n",
    "    result_df = spark.sql(f\"\"\"\n",
    "    INSERT\n",
    "    INTO `{marc_databricks_initiative#cleanse#database_name}`.`{marc_databricks_initiative#cleanse#schema_name}`.`bg_excludedrows_ent_stores_dataflow1` (\n",
    "         `bg_cleanse_id`\n",
    "    )\n",
    "    SELECT\n",
    "         `bg_cleanse_id`\n",
    "    FROM `{marc_databricks_initiative#cleanse#database_name}`.`{marc_databricks_initiative#cleanse#schema_name}`.`bg_erroneousrows_ent_stores_dataflow1`\n",
    "    \"\"\")\n",
    "    RowCountInserted = result_df.select(\"num_inserted_rows\").collect()[0][0]\n",
    "    operation_metrics = result_df.toJSON().collect()\n",
    "    operation_metrics_collection['detectduplicatedbusinesskeys_insertexcludedrows_{marc_databricks_initiative#cleanse#database_name}_{marc_databricks_initiative#cleanse#schema_name}_bg_excludedrows_ent_stores_dataflow1'] = operation_metrics\n",
    "    result_df = spark.sql(f\"\"\"\n",
    "    MERGE\n",
    "    INTO `{marc_databricks_initiative#cleanse#database_name}`.`{marc_databricks_initiative#cleanse#schema_name}`.`cls_en_ent_stores` AS `bg_target`\n",
    "    USING (\n",
    "        SELECT\n",
    "             `bg_cleanse_id`\n",
    "            ,COUNT(*) AS `bg_errorcount`\n",
    "        FROM `{marc_databricks_initiative#cleanse#database_name}`.`{marc_databricks_initiative#cleanse#schema_name}`.`bg_excludedrows_ent_stores_dataflow1`\n",
    "        GROUP BY\n",
    "             `bg_cleanse_id`\n",
    "    ) AS `bg_error`\n",
    "       ON `bg_target`.`bg_cleanse_id` = `bg_error`.`bg_cleanse_id`\n",
    "    WHEN MATCHED\n",
    "    THEN\n",
    "        UPDATE \n",
    "        SET\n",
    "             `bg_errorcount` = `bg_error`.`bg_errorcount`\n",
    "    \"\"\")\n",
    "    RowCountInserted = result_df.select(\"num_inserted_rows\").collect()[0][0]\n",
    "    operation_metrics = result_df.toJSON().collect()\n",
    "    operation_metrics_collection['updateerrorcounts_{marc_databricks_initiative#cleanse#database_name}_{marc_databricks_initiative#cleanse#schema_name}_cls_en_ent_stores'] = operation_metrics\n",
    "\n",
    "except Exception as e:\n",
    "    info('CLS_EN_Ent_stores_Action', e)\n",
    "    raise\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iEsbi2DV",
   "metadata": {},
   "outputs": [],
   "source" : [
    "dbutils.notebook.exit(operation_metrics_collection)\n"
   ]
  } ],
 "metadata": {
  "kernelspec": {
   "display_name" : "Python 3 (ipykernel)",
   "language" : "python",
   "name" : "python3"
  },
  "language_info" : {
   "codemirror_mode" : {
    "name" : "ipython",
    "version" : 3
   },
   "file_extension" : ".py",
   "mimetype" : "text/x-python",
   "name" : "python",
   "nbconvert_exporter" : "python",
   "pygments_lexer" : "ipython3",
   "version" : "3.10.9"
  }
 },
 "nbformat" : 4,
 "nbformat_minor" : 5
}
